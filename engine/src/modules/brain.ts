import type { EngineConfig } from '../config.js';
import type { BuildArtifact, BuildArtifactFile } from '../types.js';

/* ── Response shapes ─────────────────────────────────────── */

interface GroqResponse {
  choices?: Array<{ message?: { content?: string } }>;
}

interface OpenAIResponse {
  output_text?: string;
  output?: Array<{ content?: Array<{ text?: string }> }>;
}

interface AnthropicResponse {
  content?: Array<{ text?: string }>;
}

/* ── Helpers ──────────────────────────────────────────────── */

function isObject(value: unknown): value is Record<string, unknown> {
  return typeof value === 'object' && value !== null;
}

/**
 * Output Sanitization — strips markdown code fences that LLMs
 * sometimes wrap around JSON responses despite instructions.
 *
 * Handles:  ```json ... ```,  ```tsx ... ```,  ``` ... ```
 */
function sanitizeCodeFences(raw: string): string {
  let cleaned = raw.trim();

  // Remove opening fence:  ```json  /  ```tsx  /  ```typescript  /  ```
  cleaned = cleaned.replace(/^```(?:json|tsx|typescript|jsx|js|ts)?\s*\n?/i, '');

  // Remove closing fence
  cleaned = cleaned.replace(/\n?```\s*$/i, '');

  return cleaned.trim();
}

function normalizeFiles(files: unknown): BuildArtifactFile[] {
  if (!Array.isArray(files)) {
    throw new Error('LLM output does not contain a files array.');
  }

  const normalized = files
    .map(item => {
      if (!isObject(item)) return null;
      const path = item.path;
      const content = item.content;
      if (typeof path !== 'string' || typeof content !== 'string') return null;
      return { path, content };
    })
    .filter((item): item is BuildArtifactFile => item !== null);

  if (normalized.length === 0) {
    throw new Error('LLM output files array is empty or invalid.');
  }

  return normalized;
}

/**
 * Repair JSON that contains literal newlines/tabs inside string values.
 * LLMs (especially Llama) frequently put raw newlines in JSON strings
 * which breaks JSON.parse(). This walks through character-by-character
 * and escapes control characters that appear within quoted strings.
 */
function repairJson(raw: string): string {
  const result: string[] = [];
  let inString = false;
  let escaped = false;

  for (let i = 0; i < raw.length; i++) {
    const ch = raw[i];

    if (escaped) {
      result.push(ch);
      escaped = false;
      continue;
    }

    if (ch === '\\' && inString) {
      escaped = true;
      result.push(ch);
      continue;
    }

    if (ch === '"') {
      inString = !inString;
      result.push(ch);
      continue;
    }

    if (inString) {
      if (ch === '\n') {
        result.push('\\n');
        continue;
      }
      if (ch === '\r') {
        result.push('\\r');
        continue;
      }
      if (ch === '\t') {
        result.push('\\t');
        continue;
      }
    }

    result.push(ch);
  }

  return result.join('');
}

function parseArtifact(text: string): BuildArtifact {
  // Step 1: sanitize code fences first
  const clean = sanitizeCodeFences(text);

  const parseJson = (candidate: string): BuildArtifact | null => {
    // Try raw first, then repaired
    for (const attempt of [candidate, repairJson(candidate)]) {
      try {
        const parsed = JSON.parse(attempt) as unknown;
        if (!isObject(parsed)) continue;

        const projectName = typeof parsed.projectName === 'string' ? parsed.projectName : 'generated-project';
        const files = normalizeFiles(parsed.files);
        const notes = Array.isArray(parsed.notes)
          ? parsed.notes.filter((note): note is string => typeof note === 'string')
          : [];

        return { projectName, files, notes };
      } catch {
        // Try next attempt
      }
    }
    return null;
  };

  // Try direct parse after sanitization
  const direct = parseJson(clean);
  if (direct) {
    return direct;
  }

  // Fallback: extract the largest JSON block
  const jsonBlockMatch = clean.match(/\{[\s\S]*\}/);
  if (jsonBlockMatch) {
    const parsed = parseJson(jsonBlockMatch[0]);
    if (parsed) {
      return parsed;
    }
  }

  throw new Error('Unable to parse LLM response as valid artifact JSON.');
}

function createMockArtifact(prompt: string): BuildArtifact {
  return {
    projectName: 'nexus-forge-generated',
    notes: ['Generated by mock provider. Replace with real LLM credentials for production.'],
    files: [
      {
        path: 'src/main.tsx',
        content: [
          "import { StrictMode } from 'react';",
          "import { createRoot } from 'react-dom/client';",
          "import './index.css';",
          "import { App } from './App';",
          '',
          "createRoot(document.getElementById('root')!).render(",
          '  <StrictMode>',
          '    <App />',
          '  </StrictMode>,',
          ');',
        ].join('\n'),
      },
      {
        path: 'src/App.tsx',
        content: [
          "export function App() {",
          '  return (',
          '    <main className="min-h-screen p-8 bg-slate-950 text-slate-100">',
          '      <h1 className="text-4xl font-bold mb-4">Auto-Generated Submission</h1>',
          `      <p className="text-slate-300">Prompt: ${prompt.replace(/`/g, '\\`')}</p>`,
          '    </main>',
          '  );',
          '}',
        ].join('\n'),
      },
      {
        path: 'src/index.css',
        content: [
          'body {',
          '  margin: 0;',
          "  font-family: 'Inter', sans-serif;",
          '  background: #020617;',
          '  color: #f8fafc;',
          '}',
        ].join('\n'),
      },
    ],
  };
}

/* ── The Brain ───────────────────────────────────────────── */

export class Brain {
  constructor(private readonly config: EngineConfig) { }

  async generateFromPrompt(mysteryPrompt: string): Promise<BuildArtifact> {
    if (this.config.llmProvider === 'mock') {
      return createMockArtifact(mysteryPrompt);
    }

    if (!this.config.llmApiKey) {
      throw new Error('LLM_API_KEY is required when using groq/openai/anthropic provider.');
    }

    let rawText: string;

    switch (this.config.llmProvider) {
      case 'groq':
        rawText = await this.callGroq(mysteryPrompt);
        break;
      case 'openai':
        rawText = await this.callOpenAI(mysteryPrompt);
        break;
      case 'anthropic':
        rawText = await this.callAnthropic(mysteryPrompt);
        break;
      default:
        throw new Error(`Unknown LLM provider: ${this.config.llmProvider}`);
    }

    return parseArtifact(rawText);
  }

  /* ── Groq (Primary Provider) ───────────────────────────── */

  private async callGroq(mysteryPrompt: string): Promise<string> {
    const response = await fetch('https://api.groq.com/openai/v1/chat/completions', {
      method: 'POST',
      headers: {
        Authorization: `Bearer ${this.config.llmApiKey}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: this.config.llmModel,
        messages: [
          {
            role: 'system',
            content: this.config.llmSystemPrompt,
          },
          {
            role: 'user',
            content: mysteryPrompt,
          },
        ],
        temperature: this.config.llmTemperature,
        max_tokens: 8192,
      }),
    });

    const payload = (await response.json()) as GroqResponse;
    if (!response.ok) {
      throw new Error(`Groq API call failed (${response.status}): ${JSON.stringify(payload).slice(0, 500)}`);
    }

    const text = payload.choices?.[0]?.message?.content;
    if (!text || text.trim().length === 0) {
      throw new Error('Groq returned an empty response.');
    }

    return text;
  }

  /* ── OpenAI (Fallback) ─────────────────────────────────── */

  private async callOpenAI(mysteryPrompt: string): Promise<string> {
    const instruction = [
      this.config.llmSystemPrompt,
      '',
      'Mystery prompt from judges:',
      mysteryPrompt,
    ].join('\n');

    const response = await fetch('https://api.openai.com/v1/responses', {
      method: 'POST',
      headers: {
        Authorization: `Bearer ${this.config.llmApiKey}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: this.config.llmModel,
        input: [
          {
            role: 'user',
            content: [{ type: 'text', text: instruction }],
          },
        ],
        temperature: this.config.llmTemperature,
      }),
    });

    const payload = (await response.json()) as OpenAIResponse;
    if (!response.ok) {
      throw new Error(`OpenAI call failed: ${JSON.stringify(payload).slice(0, 500)}`);
    }

    const outputText = payload.output_text
      ?? payload.output?.flatMap(item => item.content ?? []).map(block => block.text ?? '').join('\n');

    if (!outputText || outputText.trim().length === 0) {
      throw new Error('OpenAI returned an empty response.');
    }

    return outputText;
  }

  /* ── Anthropic (Fallback) ──────────────────────────────── */

  private async callAnthropic(mysteryPrompt: string): Promise<string> {
    const instruction = [
      this.config.llmSystemPrompt,
      '',
      'Mystery prompt from judges:',
      mysteryPrompt,
    ].join('\n');

    const response = await fetch('https://api.anthropic.com/v1/messages', {
      method: 'POST',
      headers: {
        'x-api-key': this.config.llmApiKey ?? '',
        'anthropic-version': '2023-06-01',
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: this.config.llmModel,
        max_tokens: 4000,
        temperature: this.config.llmTemperature,
        messages: [
          {
            role: 'user',
            content: instruction,
          },
        ],
      }),
    });

    const payload = (await response.json()) as AnthropicResponse;
    if (!response.ok) {
      throw new Error(`Anthropic call failed: ${JSON.stringify(payload).slice(0, 500)}`);
    }

    const text = payload.content?.map(item => item.text ?? '').join('\n');
    if (!text || text.trim().length === 0) {
      throw new Error('Anthropic returned an empty response.');
    }

    return text;
  }
}
